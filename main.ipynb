{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b4c45d-15d0-481b-a433-5b6abb041f4b",
   "metadata": {},
   "source": [
    "# Main Process Part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997e49a2-7dc9-4cd8-ac66-85c9083542e0",
   "metadata": {},
   "source": [
    "## Introduction of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944b04f5-45cd-4d68-8b3b-e625f9884a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique Coarse Labels: ['neutral' 'fear' 'surprise' 'love' 'anger' 'sadness' 'joy']\n",
      "Unique Fine Labels: ['neutral' 'nervousness' 'curiosity' 'surprise' 'admiration' 'gratitude'\n",
      " 'anger' 'confusion' 'caring' 'disappointment' 'disapproval' 'desire'\n",
      " 'amusement' 'fear' 'sadness' 'pride' 'love' 'excitement' 'grief'\n",
      " 'realization' 'annoyance' 'approval' 'relief' 'remorse' 'embarrassment'\n",
      " 'optimism']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('labels.csv')\n",
    "\n",
    "# All coarse_label\n",
    "unique_coarse_labels = df['coarse_label'].unique()\n",
    "\n",
    "# All fine_label\n",
    "unique_fine_labels = df['fine_label'].unique()\n",
    "\n",
    "# Print\n",
    "print(\"Unique Coarse Labels:\", unique_coarse_labels)\n",
    "print(\"Unique Fine Labels:\", unique_fine_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58cc2c89-3d61-41db-9dfa-3276b5281f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a155ccec-a988-40c4-ae92-8c1b024e4325",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To keep the Rationality and Diversity of the experiment, we randomly choose 50 songs of the whole dataset,\n",
    "# which including each fine_label without repitition(if the amount of the songs including that fine_label is lower than 50, we just choose all of them)\n",
    "# This part is the code for our on-demand sampling, if you want to use it, then you can delete the markdown marks and run it.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load original label data\n",
    "df = pd.read_csv(\"labels.csv\")\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the order of fine labels, from smallest to largest\n",
    "fine_label_order = [\n",
    "    \"optimism\", \"remorse\", \"excitement\", \"embarrassment\", \"sadness\", \"disappointment\", \n",
    "    \"grief\", \"fear\", \"love\", \"relief\", \"realization\", \"disapproval\", \"anger\", \"pride\", \n",
    "    \"caring\", \"confusion\", \"surprise\", \"admiration\", \"gratitude\", \"desire\", \n",
    "    \"approval\", \"annoyance\", \"amusement\", \"curiosity\", \"nervousness\", \"neutral\"\n",
    "]\n",
    "\n",
    "selected_song_ids = set()\n",
    "final_selected_df = pd.DataFrame()\n",
    "\n",
    "for label in fine_label_order:\n",
    "    # Find all rows that have this label\n",
    "    label_rows = df[df[\"fine_label\"] == label]\n",
    "\n",
    "    # All candidate song_ids\n",
    "    candidate_song_ids = label_rows[\"song_id\"].unique()\n",
    "\n",
    "    # Remove song_ids that have already been selected\n",
    "    available_song_ids = [sid for sid in candidate_song_ids if sid not in selected_song_ids]\n",
    "\n",
    "    # Randomly select up to 50\n",
    "    if len(available_song_ids) <= 50:\n",
    "        selected_ids = available_song_ids\n",
    "    else:\n",
    "        selected_ids = np.random.choice(available_song_ids, size=50, replace=False)\n",
    "\n",
    "    # Mark as selected\n",
    "    selected_song_ids.update(selected_ids)\n",
    "\n",
    "    # Add the complete song content\n",
    "    selected_rows = df[df[\"song_id\"].isin(selected_ids)]\n",
    "    final_selected_df = pd.concat([final_selected_df, selected_rows], ignore_index=True)\n",
    "\n",
    "# Save the results\n",
    "final_selected_df.to_csv(\"experiment.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c246e0-39aa-4bbe-b59e-d0d00e00dce1",
   "metadata": {},
   "source": [
    "## Main Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db224a4d-6f8e-43ad-a6f9-3247f28dd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For some Recommended Version of some key modules: numpy==1.24.4, matplotlib==3.7.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0298829-1f1f-47f8-8e3d-0558292badc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Training: StructFormer-Hyper\n",
      "[StructFormer-Hyper] Epoch 1: Loss = 2.7318\n",
      "[StructFormer-Hyper] Epoch 2: Loss = 2.1837\n",
      "[StructFormer-Hyper] Epoch 3: Loss = 1.7842\n",
      "[StructFormer-Hyper] Epoch 4: Loss = 1.4584\n",
      "[StructFormer-Hyper] Epoch 5: Loss = 1.1952\n",
      "[StructFormer-Hyper] Epoch 6: Loss = 0.9945\n",
      "[StructFormer-Hyper] Epoch 7: Loss = 0.8464\n",
      "[StructFormer-Hyper] Epoch 8: Loss = 0.7241\n",
      "[StructFormer-Hyper] Epoch 9: Loss = 0.6354\n",
      "[StructFormer-Hyper] Epoch 10: Loss = 0.5686\n",
      "[StructFormer-Hyper] Epoch 11: Loss = 0.5045\n",
      "[StructFormer-Hyper] Epoch 12: Loss = 0.4578\n",
      "[StructFormer-Hyper] Epoch 13: Loss = 0.4160\n",
      "[StructFormer-Hyper] Epoch 14: Loss = 0.3809\n",
      "[StructFormer-Hyper] Epoch 15: Loss = 0.3511\n",
      "[StructFormer-Hyper] Epoch 16: Loss = 0.3271\n",
      "[StructFormer-Hyper] Epoch 17: Loss = 0.3032\n",
      "[StructFormer-Hyper] Epoch 18: Loss = 0.2792\n",
      "[StructFormer-Hyper] Epoch 19: Loss = 0.2641\n",
      "[StructFormer-Hyper] Epoch 20: Loss = 0.2463\n",
      "📊 Eval for StructFormer-Hyper: {'macro_f1': 0.6530178202889233, 'weighted_f1': 0.9621420702165754, 'hierarchical_macro_f1': 0.2012910799223492, 'silhouette_score': 0.8025909, 'loss_per_coarse_label': {0: 2.6086218504505303, 6: 0.13031296024144506, 5: 2.574318737502492, 1: 3.5941198760387945, 4: 1.7389015496876847, 3: 0.4052873638344109, 2: 0.9718551642055124}, 'macro_f1_per_coarse_label': {0: 0.2323246351918581, 6: 0.12894038926094295, 5: 0.2550370243937663, 1: 0.13173890063424948, 4: 0.2684395695319995, 3: 0.12795013147634057, 2: 0.26460690896728745}, 'sentiment_weighted_f1': {'positive': 0.8705980806780333, 'neutral': 0.9794304851189346, 'negative': 0.9463828155770373}}\n",
      "\n",
      "🚀 Training: BERT\n",
      "[BERT] Epoch 1: Loss = 0.3812\n",
      "[BERT] Epoch 2: Loss = 0.1728\n",
      "[BERT] Epoch 3: Loss = 0.1109\n",
      "[BERT] Epoch 4: Loss = 0.0722\n",
      "[BERT] Epoch 5: Loss = 0.0488\n",
      "[BERT] Epoch 6: Loss = 0.0308\n",
      "[BERT] Epoch 7: Loss = 0.0230\n",
      "[BERT] Epoch 8: Loss = 0.0206\n",
      "[BERT] Epoch 9: Loss = 0.0150\n",
      "[BERT] Epoch 10: Loss = 0.0103\n",
      "[BERT] Epoch 11: Loss = 0.0108\n",
      "[BERT] Epoch 12: Loss = 0.0156\n",
      "[BERT] Epoch 13: Loss = 0.0086\n",
      "[BERT] Epoch 14: Loss = 0.0074\n",
      "[BERT] Epoch 15: Loss = 0.0083\n",
      "[BERT] Epoch 16: Loss = 0.0086\n",
      "[BERT] Epoch 17: Loss = 0.0050\n",
      "[BERT] Epoch 18: Loss = 0.0063\n",
      "[BERT] Epoch 19: Loss = 0.0060\n",
      "[BERT] Epoch 20: Loss = 0.0067\n",
      "📊 Eval for BERT: {'macro_f1': 0.7264625484125194, 'weighted_f1': 0.9641746646742466, 'hierarchical_macro_f1': 0.23208658728061074, 'silhouette_score': 0.54983014, 'loss_per_coarse_label': {0: 1.2299939788735332, 6: 0.12067811463391297, 5: 1.1050897266924564, 1: 3.033813874292879, 4: 0.9804024862599232, 3: 0.2514897746356256, 2: 0.5435258306997968}, 'macro_f1_per_coarse_label': {0: 0.24609668806097376, 6: 0.1189344323910185, 5: 0.28354857956183555, 1: 0.18296893973146036, 4: 0.29537908321908574, 3: 0.24883493653611075, 2: 0.24884345146379044}, 'sentiment_weighted_f1': {'positive': 0.8877852138183212, 'neutral': 0.9797049729780901, 'negative': 0.9583744676092566}}\n",
      "\n",
      "🚀 Training: Multi-task BERT\n",
      "[Multi-task BERT] Epoch 1: Loss = 0.5318\n",
      "[Multi-task BERT] Epoch 2: Loss = 0.2361\n",
      "[Multi-task BERT] Epoch 3: Loss = 0.1481\n",
      "[Multi-task BERT] Epoch 4: Loss = 0.1018\n",
      "[Multi-task BERT] Epoch 5: Loss = 0.0718\n",
      "[Multi-task BERT] Epoch 6: Loss = 0.0500\n",
      "[Multi-task BERT] Epoch 7: Loss = 0.0360\n",
      "[Multi-task BERT] Epoch 8: Loss = 0.0231\n",
      "[Multi-task BERT] Epoch 9: Loss = 0.0228\n",
      "[Multi-task BERT] Epoch 10: Loss = 0.0186\n",
      "[Multi-task BERT] Epoch 11: Loss = 0.0196\n",
      "[Multi-task BERT] Epoch 12: Loss = 0.0135\n",
      "[Multi-task BERT] Epoch 13: Loss = 0.0121\n",
      "[Multi-task BERT] Epoch 14: Loss = 0.0095\n",
      "[Multi-task BERT] Epoch 15: Loss = 0.0128\n",
      "[Multi-task BERT] Epoch 16: Loss = 0.0093\n",
      "[Multi-task BERT] Epoch 17: Loss = 0.0115\n",
      "[Multi-task BERT] Epoch 18: Loss = 0.0063\n",
      "[Multi-task BERT] Epoch 19: Loss = 0.0045\n",
      "[Multi-task BERT] Epoch 20: Loss = 0.0078\n",
      "📊 Eval for Multi-task BERT: {'macro_f1': 0.7164367458620149, 'weighted_f1': 0.9643970362369023, 'hierarchical_macro_f1': 0.22251353205710991, 'silhouette_score': 0.59825224, 'loss_per_coarse_label': {0: 1.1978809306840146, 6: 0.09527402160155625, 5: 1.528397310176093, 1: 2.6324667407493063, 4: 0.9494164165705864, 3: 0.32243507157356766, 2: 1.1598629430653022}, 'macro_f1_per_coarse_label': {0: 0.2457276262371111, 6: 0.12474718908094914, 5: 0.30111247417407866, 1: 0.248223474638569, 4: 0.30074124826696935, 3: 0.16529378110875909, 2: 0.17174893089333299}, 'sentiment_weighted_f1': {'positive': 0.8874682720664006, 'neutral': 0.982248988722753, 'negative': 0.9385326278968887}}\n",
      "\n",
      "🚀 Training: HAN\n",
      "[HAN] Epoch 1: Loss = 2.4737\n",
      "[HAN] Epoch 2: Loss = 0.9057\n",
      "[HAN] Epoch 3: Loss = 0.7948\n",
      "[HAN] Epoch 4: Loss = 0.7204\n",
      "[HAN] Epoch 5: Loss = 0.6486\n",
      "[HAN] Epoch 6: Loss = 0.5890\n",
      "[HAN] Epoch 7: Loss = 0.5424\n",
      "[HAN] Epoch 8: Loss = 0.5080\n",
      "[HAN] Epoch 9: Loss = 0.4837\n",
      "[HAN] Epoch 10: Loss = 0.4655\n",
      "[HAN] Epoch 11: Loss = 0.4511\n",
      "[HAN] Epoch 12: Loss = 0.4394\n",
      "[HAN] Epoch 13: Loss = 0.4282\n",
      "[HAN] Epoch 14: Loss = 0.4178\n",
      "[HAN] Epoch 15: Loss = 0.4063\n",
      "[HAN] Epoch 16: Loss = 0.3962\n",
      "[HAN] Epoch 17: Loss = 0.3858\n",
      "[HAN] Epoch 18: Loss = 0.3772\n",
      "[HAN] Epoch 19: Loss = 0.3704\n",
      "[HAN] Epoch 20: Loss = 0.3640\n",
      "📊 Eval for HAN: {'macro_f1': 0.32033276680173384, 'weighted_f1': 0.9113166579829071, 'hierarchical_macro_f1': 0.25457679338800904, 'silhouette_score': -0.11663152, 'loss_per_coarse_label': {0: 3.178306386306996, 6: 0.11262337318911701, 5: 3.6896656518955844, 1: 6.367616406842774, 4: 1.6761945369388784, 3: 0.5553962954510379, 2: 1.1824002888346286}, 'macro_f1_per_coarse_label': {0: 0.16554054054054054, 6: 0.4994488759317611, 5: 0.2622902836378122, 1: 0.0, 4: 0.37511873308154897, 3: 0.23616085965483555, 2: 0.24347826086956523}, 'sentiment_weighted_f1': {'positive': 0.4601614863042246, 'neutral': 0.9619858332732559, 'negative': 0.8384499210064847}}\n"
     ]
    }
   ],
   "source": [
    "#for main experiment models\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "# ✅ Training function\n",
    "from structure_model import StructureAwareClassifier, HANClassifier, compute_total_loss, extended_eval\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ✅ Global config\n",
    "NUM_SONGS = 1041 # 1041 is the amount of all the songs in experiment.csv, you can adjust it as you need\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_FRAC = 0.2\n",
    "VAL_FRAC = 0.1\n",
    "SEED = 42\n",
    "COARSE_LABEL_NAMES = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"love\", \"neutral\"]\n",
    "\n",
    "def seed_all(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "\n",
    "# ✅ Coarse label mapping\n",
    "coarse2id = {label: i for i, label in enumerate(COARSE_LABEL_NAMES)}\n",
    "\n",
    "# ✅ Load + encode dataset\n",
    "df = pd.read_csv(\"experiment.csv\")\n",
    "fine_labels = sorted(df[\"fine_label\"].unique())\n",
    "fine2id = {label: i for i, label in enumerate(fine_labels)}\n",
    "df[\"coarse_label_id\"] = df[\"coarse_label\"].map(coarse2id)\n",
    "df[\"fine_label_id\"] = df[\"fine_label\"].map(fine2id)\n",
    "\n",
    "NUM_FINE_CLASSES = len(fine2id)\n",
    "\n",
    "# ✅ Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-uncased\", local_files_only=True)\n",
    "\n",
    "# ✅ Dataset wrapper\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        encoded = tokenizer(row[\"line_text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"fine_label\": torch.tensor(row[\"fine_label_id\"]),\n",
    "            \"coarse_label\": torch.tensor(row[\"coarse_label_id\"], dtype=torch.long),\n",
    "            \"para_id\": torch.tensor(row[\"para_id\"]),\n",
    "            \"line_id\": torch.tensor(row[\"line_id\"]),\n",
    "        }\n",
    "\n",
    "# ✅ Dataset split\n",
    "unique_ids = df[\"song_id\"].unique()[:NUM_SONGS]\n",
    "df = df[df[\"song_id\"].isin(unique_ids)].reset_index(drop=True)\n",
    "train_val = df.sample(frac=TRAIN_FRAC + VAL_FRAC, random_state=SEED)\n",
    "train_df = train_val.sample(frac=TRAIN_FRAC / (TRAIN_FRAC + VAL_FRAC), random_state=SEED)\n",
    "val_df = train_val.drop(train_df.index)\n",
    "test_df = df.drop(train_val.index)\n",
    "\n",
    "train_loader = DataLoader(LyricsDataset(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(LyricsDataset(val_df), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(LyricsDataset(test_df), batch_size=BATCH_SIZE)\n",
    "\n",
    "# ✅ Fine-to-coarse mapping\n",
    "def get_fine_to_coarse_map(df):\n",
    "    return dict(zip(df[\"fine_label_id\"], df[\"coarse_label_id\"]))\n",
    "fine_to_coarse_map = get_fine_to_coarse_map(df)\n",
    "\n",
    "\n",
    "# ✅ Train function with support for models\n",
    "def train_model(model, train_loader, name, cfg, epochs=5):\n",
    "    import geoopt\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.device = device\n",
    "\n",
    "    # ✅ Use Riemannian optimizer for StructFormer-Hyper only\n",
    "    if name == \"StructFormer-Hyper\":\n",
    "        optimizer = geoopt.optim.RiemannianAdam(model.parameters(), lr=1e-5)\n",
    "    elif isinstance(model, HANClassifier):\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    else:\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "    results = {\"train_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            coarse_logits, fine_logits, struct_logits = model(\n",
    "                batch[\"input_ids\"], batch[\"attention_mask\"],\n",
    "                batch[\"para_id\"], batch[\"line_id\"]\n",
    "            )\n",
    "            z_euc, z_hyp = model.get_embeddings(\n",
    "                batch[\"input_ids\"], batch[\"attention_mask\"],\n",
    "                batch[\"para_id\"], batch[\"line_id\"]\n",
    "            )\n",
    "            losses = compute_total_loss(\n",
    "                fine_logits, batch[\"fine_label\"],\n",
    "                coarse_logits, batch[\"coarse_label\"],\n",
    "                struct_logits=struct_logits,\n",
    "                para_ids=batch[\"para_id\"],\n",
    "                z_euc=z_euc, z_hyp=z_hyp,\n",
    "                lambda_fine=1.0, lambda_coarse=0.5,\n",
    "                lambda_align=0.4, lambda_geom=0.05, margin=1.0,\n",
    "                model=model\n",
    "            )\n",
    "            loss = losses[\"total\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total += loss.item()\n",
    "        results[\"train_loss\"].append(total / len(train_loader))\n",
    "        print(f\"[{name}] Epoch {epoch+1}: Loss = {results['train_loss'][-1]:.4f}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "# ✅ Model configuration: five variants for ablation and full-model comparison\n",
    "model_configs = {\n",
    "    # StructFormer-Hyper: Full model with structure, projection, hierarchy, and hyperbolic geometry\n",
    "    \"StructFormer-Hyper\": dict(\n",
    "        use_structure=True, use_hyperbolic=True,\n",
    "        project_structure=True, use_coarse_supervision=True,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "    # B1: Baseline BERT without structure, hyperbolic geometry, or coarse supervision\n",
    "    \"BERT\": dict(\n",
    "        use_structure=False, use_hyperbolic=False,\n",
    "        project_structure=False, use_coarse_supervision=False,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "    # M2: Adds paragraph and line IDs without projection, no hierarchy or hyperbolic space\n",
    "    \"Multi-task BERT\": dict(\n",
    "        use_structure=False, use_hyperbolic=False,\n",
    "        project_structure=False, use_coarse_supervision=True,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "    # H2: Uses only hyperbolic geometry and coarse label supervision, no structural input\n",
    "    \"HAN\": dict(model_type=\"han\", num_fine_labels=NUM_FINE_CLASSES)\n",
    "}\n",
    "\n",
    "# ✅ Train and evaluate\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# ✅ Updated model loading logic\n",
    "for name, cfg in model_configs.items():\n",
    "    print(f\"\\n🚀 Training: {name}\")\n",
    "    if cfg.get(\"model_type\") == \"han\":\n",
    "        model = HANClassifier(num_classes=NUM_FINE_CLASSES)\n",
    "    else:\n",
    "        model = StructureAwareClassifier(**cfg)\n",
    "    train_result = train_model(model, train_loader, name, cfg, epochs=EPOCHS)\n",
    "    results[name] = train_result\n",
    "    trained_models[name] = model\n",
    "    print(f\"📊 Eval for {name}:\", extended_eval(model, test_loader, fine_to_coarse_map))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d9932a-f400-45bd-a8dd-e5e8148b57ae",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee852d68-dcfa-4c09-8e05-5626ec4ebd2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: The TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\n",
      "  warnings.warn(problem)\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/root/miniconda3/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Visualization\n",
    "from visualization import (\n",
    "    plot_all_model_heatmaps,\n",
    "    plot_all_model_umaps,\n",
    "    plot_all_model_norm_trends\n",
    ")\n",
    "\n",
    "# ✅ define label\n",
    "coarse_label_names = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"love\", \"neutral\"]\n",
    "fine_label_names = [\n",
    "    \"joy\", \"amusement\", \"pride\", \"excitement\", \"relief\", \"optimism\",\n",
    "    \"sadness\", \"grief\", \"disappointment\", \"remorse\",\n",
    "    \"anger\", \"annoyance\", \"disapproval\",\n",
    "    \"fear\", \"embarrassment\", \"nervousness\",\n",
    "    \"surprise\", \"realization\", \"confusion\",\n",
    "    \"love\", \"gratitude\", \"desire\",\n",
    "    \"neutral\", \"curiosity\", \"approval\", \"admiration\"\n",
    "]\n",
    "\n",
    "# ✅ heatmap\n",
    "plot_all_model_heatmaps(\n",
    "    models_dict=trained_models,\n",
    "    loader=test_loader,\n",
    "    fine_label_names=fine_label_names,\n",
    "    coarse_label_names=coarse_label_names,\n",
    "    save_path=\"viz_heatmaps_all_models.png\"\n",
    ")\n",
    "\n",
    "# ✅ UMAP\n",
    "plot_all_model_umaps(\n",
    "    models_dict=trained_models,\n",
    "    loader=test_loader,\n",
    "    fine_label_names=fine_label_names,\n",
    "    save_path=\"viz_umap_all_models.png\"\n",
    ")\n",
    "\n",
    "# ✅ Norm trends\n",
    "plot_all_model_norm_trends(\n",
    "    models_dict=trained_models,\n",
    "    loader=test_loader,\n",
    "    save_path=\"viz_norm_trends_all_models.png\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4055669-771d-492e-9228-c35e583c50b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Plot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "# Set the style to the style used by NeurIPS\n",
    "plt.style.use('seaborn')\n",
    "sns.set_context(\"paper\", font_scale=1.4)\n",
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.rcParams['grid.alpha'] = 0.3\n",
    "\n",
    "# Prepare data (loss values extracted from the document)\n",
    "models = { }\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(10, 6), dpi=300)\n",
    "colors = sns.color_palette(\"husl\", 4)\n",
    "linestyles = ['-', '--', '-.', ':']\n",
    "markers = ['o', 's', 'D', '^']\n",
    "\n",
    "for idx, (model_name, losses) in enumerate(models.items()):\n",
    "    epochs = np.arange(1, len(losses)+1)\n",
    "    plt.plot(epochs, losses, \n",
    "             label=model_name, \n",
    "             color=colors[idx],\n",
    "             linestyle=linestyles[idx],\n",
    "             marker=markers[idx],\n",
    "             markersize=6,\n",
    "             linewidth=2,\n",
    "             markevery=2)\n",
    "\n",
    "# Add final evaluation weighted-F1 values as annotations\n",
    "final_f1 = {\n",
    "    \"StructFormer-Hyper\": 0.96,\n",
    "    \"BERT\": 0.96,\n",
    "    \"Multi-task BERT\": 0.96,\n",
    "    \"HAN\": 0.91\n",
    "}\n",
    "\n",
    "for idx, (model_name, f1) in enumerate(final_f1.items()):\n",
    "    y_pos = models[model_name][-1]\n",
    "    plt.annotate(f'Weighted-F1: {f1:.2f}',\n",
    "                 xy=(20, y_pos),\n",
    "                 xytext=(22, y_pos),\n",
    "                 color=colors[idx],\n",
    "                 fontsize=10,\n",
    "                 arrowprops=dict(arrowstyle=\"->\", color=colors[idx]))\n",
    "\n",
    "# Decorate the plot\n",
    "plt.title('Training Loss Curves (20 Epochs)', pad=20)\n",
    "plt.xlabel('Epoch', labelpad=10)\n",
    "plt.ylabel('Loss Value', labelpad=10)\n",
    "plt.xticks(np.arange(0, 21, 2))\n",
    "plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "plt.legend(loc='upper right', framealpha=1)\n",
    "plt.yscale('log')  # Use logarithmic scale to better display loss values across different orders of magnitude\n",
    "\n",
    "# Add NeurIPS-style grid and borders\n",
    "sns.despine()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.2)\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "#plt.savefig('training_loss_curves.pdf', bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558b79f6-0b4a-4bc8-8bb1-e56402c0a582",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import ttest_rel\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Step 1: extract embeddings and labels\n",
    "def extract_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    embeddings, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            para_id = batch[\"para_id\"].to(model.device)\n",
    "            line_id = batch[\"line_id\"].to(model.device)\n",
    "            fine_label = batch[\"fine_label\"]\n",
    "            z_euc, _ = model.get_embeddings(input_ids, attn_mask, para_id, line_id)\n",
    "            embeddings.append(z_euc.cpu().numpy())\n",
    "            labels.append(fine_label.cpu().numpy())\n",
    "    return np.concatenate(embeddings), np.concatenate(labels)\n",
    "\n",
    "# Step 2: paired bootstrap sampling\n",
    "def bootstrap_silhouette_paired(X1, y1, X2, y2, n_iter=500, sample_size=512, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    scores1, scores2 = [], []\n",
    "    N = len(X1)\n",
    "    sample_size = min(sample_size, N)\n",
    "    for _ in range(n_iter):\n",
    "        idx = np.random.choice(N, size=sample_size, replace=True)\n",
    "        scores1.append(silhouette_score(X1[idx], y1[idx]))\n",
    "        scores2.append(silhouette_score(X2[idx], y2[idx]))\n",
    "    return np.array(scores1), np.array(scores2)\n",
    "\n",
    "# Step 3: paired t-test comparison\n",
    "def compare_models(model_main, model_base, loader, base_name, n_iter=500, sample_size=512):\n",
    "    print(f\"\\n🔍 Testing StructFormer-Hyper vs {base_name}\")\n",
    "    X_main, y_main = extract_embeddings(model_main, loader)\n",
    "    X_base, y_base = extract_embeddings(model_base, loader)\n",
    "    scores_main, scores_base = bootstrap_silhouette_paired(\n",
    "        X_main, y_main, X_base, y_base, n_iter=n_iter, sample_size=sample_size\n",
    "    )\n",
    "    t_stat, p_val = ttest_rel(scores_main, scores_base)\n",
    "    print(f\"Paired t-test: t = {t_stat:.4f}, p = {p_val:.4f}\")\n",
    "    return scores_main, scores_base, p_val\n",
    "\n",
    "# Dictionary of trained models\n",
    "main_model = trained_models[\"StructFormer-Hyper\"]\n",
    "baselines = [\"BERT\", \"Multi-task BERT\", \"HAN\"]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for base_name in baselines:\n",
    "    base_model = trained_models[base_name]\n",
    "    scores_main, scores_base, p_val = compare_models(\n",
    "        model_main=main_model,\n",
    "        model_base=base_model,\n",
    "        loader=test_loader,\n",
    "        base_name=base_name,\n",
    "        n_iter=500,\n",
    "        sample_size=512\n",
    "    )\n",
    "    results[base_name] = {\n",
    "        \"StructFormer-Hyper\": scores_main,\n",
    "        base_name: scores_base,\n",
    "        \"p-value\": p_val\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8416cf-fe1d-4a4f-9919-ecf7e7022a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Wilcoxon Test: StructFormer-Hyper vs BERT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.stats import wilcoxon\n",
    "import torch\n",
    "\n",
    "# Function to extract embeddings and labels\n",
    "def extract_embeddings(model, loader):\n",
    "    model.eval()\n",
    "    embeddings, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            input_ids = batch[\"input_ids\"].to(model.device)\n",
    "            attn_mask = batch[\"attention_mask\"].to(model.device)\n",
    "            para_id = batch[\"para_id\"].to(model.device)\n",
    "            line_id = batch[\"line_id\"].to(model.device)\n",
    "            fine_label = batch[\"fine_label\"]\n",
    "            z_euc, _ = model.get_embeddings(input_ids, attn_mask, para_id, line_id)\n",
    "            embeddings.append(z_euc.cpu().numpy())\n",
    "            labels.append(fine_label.cpu().numpy())\n",
    "    return np.concatenate(embeddings), np.concatenate(labels)\n",
    "\n",
    "# Bootstrap silhouette using same indices for both models\n",
    "def bootstrap_silhouette_paired(X1, y1, X2, y2, n_iter=500, sample_size=None, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    scores1, scores2 = [], []\n",
    "    N = len(X1)\n",
    "    if sample_size is None or sample_size > N:\n",
    "        sample_size = N\n",
    "\n",
    "    for _ in range(n_iter):\n",
    "        idx = np.random.choice(N, size=sample_size, replace=True)\n",
    "        scores1.append(silhouette_score(X1[idx], y1[idx]))\n",
    "        scores2.append(silhouette_score(X2[idx], y2[idx]))\n",
    "    return np.array(scores1), np.array(scores2)\n",
    "\n",
    "# Compare models using Wilcoxon signed-rank test\n",
    "def compare_models_wilcoxon(model_main, model_base, loader, base_name, n_iter=500, sample_size=None):\n",
    "    print(f\"\\n🔍 Wilcoxon Test: StructFormer-Hyper vs {base_name}\")\n",
    "    X_main, y_main = extract_embeddings(model_main, loader)\n",
    "    X_base, y_base = extract_embeddings(model_base, loader)\n",
    "\n",
    "    scores_main, scores_base = bootstrap_silhouette_paired(\n",
    "        X_main, y_main, X_base, y_base, n_iter=n_iter, sample_size=sample_size\n",
    "    )\n",
    "\n",
    "    stat, p_val = wilcoxon(scores_main, scores_base)\n",
    "    print(f\"Wilcoxon signed-rank test: statistic = {stat:.4f}, p = {p_val:.4f}\")\n",
    "    return scores_main, scores_base, p_val\n",
    "\n",
    "# Running comparisons for all baselines\n",
    "main_model = trained_models[\"StructFormer-Hyper\"]\n",
    "baselines = [\"BERT\", \"Multi-task BERT\", \"HAN\"]\n",
    "\n",
    "results_wilcoxon = {}\n",
    "\n",
    "for base_name in baselines:\n",
    "    base_model = trained_models[base_name]\n",
    "    scores_main, scores_base, p_val = compare_models_wilcoxon(\n",
    "        main_model, base_model, test_loader, base_name=base_name, n_iter=500, sample_size=None\n",
    "    )\n",
    "    results_wilcoxon[base_name] = {\n",
    "        \"StructFormer-Hyper\": scores_main,\n",
    "        base_name: scores_base,\n",
    "        \"p-value\": p_val\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70747999-1bb7-49a6-8a75-47810fd9a44a",
   "metadata": {},
   "source": [
    "## Ablation Studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768d898c-648d-40b5-841a-9643dd443cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "# ✅ Training function\n",
    "from structure_model import StructureAwareClassifier, HANClassifier, compute_total_loss, extended_eval\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# ✅ Global config\n",
    "NUM_SONGS = 300 # 1041 is the amount of all the songs in experiment.csv, you can adjust it as you need\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 16\n",
    "TRAIN_FRAC = 0.2\n",
    "VAL_FRAC = 0.1\n",
    "SEED = 42\n",
    "COARSE_LABEL_NAMES = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"love\", \"neutral\"]\n",
    "\n",
    "def seed_all(seed=SEED):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_all()\n",
    "\n",
    "# ✅ Coarse label mapping\n",
    "coarse2id = {label: i for i, label in enumerate(COARSE_LABEL_NAMES)}\n",
    "\n",
    "# ✅ Load + encode dataset\n",
    "df = pd.read_csv(\"experiment.csv\")\n",
    "fine_labels = sorted(df[\"fine_label\"].unique())\n",
    "fine2id = {label: i for i, label in enumerate(fine_labels)}\n",
    "df[\"coarse_label_id\"] = df[\"coarse_label\"].map(coarse2id)\n",
    "df[\"fine_label_id\"] = df[\"fine_label\"].map(fine2id)\n",
    "\n",
    "NUM_FINE_CLASSES = len(fine2id)\n",
    "\n",
    "# ✅ Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./bert-base-uncased\", local_files_only=True)\n",
    "\n",
    "# ✅ Dataset wrapper\n",
    "class LyricsDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        encoded = tokenizer(row[\"line_text\"], padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"fine_label\": torch.tensor(row[\"fine_label_id\"]),\n",
    "            \"coarse_label\": torch.tensor(row[\"coarse_label_id\"], dtype=torch.long),\n",
    "            \"para_id\": torch.tensor(row[\"para_id\"]),\n",
    "            \"line_id\": torch.tensor(row[\"line_id\"]),\n",
    "        }\n",
    "\n",
    "# ✅ Dataset split\n",
    "unique_ids = df[\"song_id\"].unique()[:NUM_SONGS]\n",
    "df = df[df[\"song_id\"].isin(unique_ids)].reset_index(drop=True)\n",
    "train_val = df.sample(frac=TRAIN_FRAC + VAL_FRAC, random_state=SEED)\n",
    "train_df = train_val.sample(frac=TRAIN_FRAC / (TRAIN_FRAC + VAL_FRAC), random_state=SEED)\n",
    "val_df = train_val.drop(train_df.index)\n",
    "test_df = df.drop(train_val.index)\n",
    "\n",
    "train_loader = DataLoader(LyricsDataset(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(LyricsDataset(val_df), batch_size=BATCH_SIZE)\n",
    "test_loader = DataLoader(LyricsDataset(test_df), batch_size=BATCH_SIZE)\n",
    "\n",
    "# ✅ Fine-to-coarse mapping\n",
    "def get_fine_to_coarse_map(df):\n",
    "    return dict(zip(df[\"fine_label_id\"], df[\"coarse_label_id\"]))\n",
    "fine_to_coarse_map = get_fine_to_coarse_map(df)\n",
    "\n",
    "\n",
    "# ✅ Train function with support for both models\n",
    "def train_model(model, train_loader, name, cfg, epochs=5):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.device = device\n",
    "    optimizer = (\n",
    "        torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
    "        if not isinstance(model, HANClassifier)\n",
    "        else torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    )\n",
    "    results = {\"train_loss\": []}\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total = 0\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            coarse_logits, fine_logits, struct_logits = model(\n",
    "                batch[\"input_ids\"], batch[\"attention_mask\"],\n",
    "                batch[\"para_id\"], batch[\"line_id\"]\n",
    "            )\n",
    "            z_euc, z_hyp = model.get_embeddings(\n",
    "                batch[\"input_ids\"], batch[\"attention_mask\"],\n",
    "                batch[\"para_id\"], batch[\"line_id\"]\n",
    "            )\n",
    "            losses = compute_total_loss(\n",
    "                fine_logits, batch[\"fine_label\"],\n",
    "                coarse_logits, batch[\"coarse_label\"],\n",
    "                struct_logits=struct_logits,\n",
    "                para_ids=batch[\"para_id\"],\n",
    "                z_euc=z_euc, z_hyp=z_hyp,\n",
    "                lambda_fine=1.0, lambda_coarse=0.5,\n",
    "                lambda_align=0.2, lambda_geom=0.5, margin=1.0,\n",
    "                model=model\n",
    "            )\n",
    "            loss = losses[\"total\"]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            total += loss.item()\n",
    "        results[\"train_loss\"].append(total / len(train_loader))\n",
    "        print(f\"[{name}] Epoch {epoch+1}: Loss = {results['train_loss'][-1]:.4f}\")\n",
    "    return results\n",
    "\n",
    "# ✅ Model configuration: five variants for ablation and full-model comparison\n",
    "# ✅ Ablation experiment configuration\n",
    "model_configs = {\n",
    "    # Full model with structure, projection, coarse supervision, and hyperbolic geometry\n",
    "    \"StructFormer-Hyper\": dict(\n",
    "        use_structure=True,\n",
    "        use_hyperbolic=True,\n",
    "        project_structure=True,\n",
    "        use_coarse_supervision=True,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "\n",
    "    # Remove structural information: no paragraph/line embeddings\n",
    "    \"StructFormer-NoStruct\": dict(\n",
    "        use_structure=False,\n",
    "        use_hyperbolic=True,\n",
    "        project_structure=False,\n",
    "        use_coarse_supervision=True,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "\n",
    "    # Remove hyperbolic geometry: model in Euclidean space only\n",
    "    \"StructFormer-NoHyper\": dict(\n",
    "        use_structure=True,\n",
    "        use_hyperbolic=False,\n",
    "        project_structure=True,\n",
    "        use_coarse_supervision=True,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "\n",
    "    # Remove structure projection: structure is not mapped to BERT hidden space\n",
    "    \"StructFormer-NoProj\": dict(\n",
    "        use_structure=True,\n",
    "        use_hyperbolic=True,\n",
    "        project_structure=False,\n",
    "        use_coarse_supervision=True,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "\n",
    "    # Remove coarse label supervision: no auxiliary supervision at coarse level\n",
    "    \"StructFormer-NoCoarse\": dict(\n",
    "        use_structure=True,\n",
    "        use_hyperbolic=True,\n",
    "        project_structure=True,\n",
    "        use_coarse_supervision=False,\n",
    "        num_fine_labels=NUM_FINE_CLASSES\n",
    "    ),\n",
    "}\n",
    "\n",
    "# ✅ Train and evaluate\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "# ✅ Updated model loading logic\n",
    "for name, cfg in model_configs.items():\n",
    "    print(f\"\\n🚀 Training: {name}\")\n",
    "    if cfg.get(\"model_type\") == \"han\":\n",
    "        model = HANClassifier(num_classes=NUM_FINE_CLASSES)\n",
    "    else:\n",
    "        model = StructureAwareClassifier(**cfg)\n",
    "    train_result = train_model(model, train_loader, name, cfg, epochs=EPOCHS)\n",
    "    results[name] = train_result\n",
    "    trained_models[name] = model\n",
    "    print(f\"📊 Eval for {name}:\", extended_eval(model, test_loader, fine_to_coarse_map))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
